{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I have analyzed the OpenStreetMap data for the [San Francisco](http://www.openstreetmap.org/relation/111968) city area. I have broken down the analysis into four parts-\n",
    "\n",
    "* __Data Extraction__\n",
    "* __Data Cleaning and transformation__\n",
    "* __Uploading to database__\n",
    "* __Clustering Analysis__\n",
    "\n",
    "__Data Extraction:__ I have downloaded the OSM XML file from OpenStreetMap for the San Francisco city area. Since the file is 82 MB in size, I have created a smaller sample file for looking at the data and get an idea of what fields need to be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET    # For parsing XML \n",
    "\n",
    "k = 20   # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(input_filename):\n",
    "    tags = ('node', 'way', 'relation')\n",
    "    # Yield element if it is the right type of tag    \n",
    "    context = iter(ET.iterparse(input_filename, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def create_sample(input_filename, output_sample_filename):\n",
    "    with open(output_sample_filename, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "        # Write every kth top level element\n",
    "        for i, element in enumerate(get_element(input_filename)):\n",
    "            if i % k == 0:\n",
    "                output.write(ET.tostring(element, encoding='utf-8'))\n",
    "        output.write('</osm>')\n",
    "\n",
    "create_sample('san-francisco.osm', 'sample_sf.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Structure of the XML file_ - It is a list of instances of data primitives (nodes, ways, and relations).\n",
    "* _Node_ - A single point in space defined by its latitude, longitude and node id. Nodes can be used to define point features, and will have one or several tags to define the typ of feature. For example in this case, nodes are used to define restaurants and will have several tags for cuisine type, name, etc. \n",
    "* _Way_ - A way is an ordered list of nodes which normally also has at least one tag.\n",
    "* _Relation_ - It is a logical or geographic relationships between objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 3118,\n",
      " 'nd': 362013,\n",
      " 'node': 307268,\n",
      " 'osm': 1,\n",
      " 'relation': 300,\n",
      " 'tag': 98594,\n",
      " 'way': 37584}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Use iterative parsing to process the map file to find out what and how many tags are there\n",
    "# Get an idea on how much of which data you can expect to have in the map\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    # Returns a dictionary with the tag name as the key and number of times this tag can be encountered in the map as value.\n",
    "    tag_count = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tag_count:\n",
    "            tag_count[elem.tag] = 0\n",
    "        tag_count[elem.tag] = tag_count[elem.tag] + 1   \n",
    "    return tag_count\n",
    "        \n",
    "pprint.pprint(count_tags('sample_sf.osm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def get_user(element):\n",
    "    userid = element.get(\"uid\")\n",
    "    return userid\n",
    "\n",
    "def process_users(filename):    # return a set of unique user IDs (\"uid\")\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        uid = \"uid\"\n",
    "        if uid in element.keys():\n",
    "            userid = get_user(element)\n",
    "            users.add(userid)\n",
    "    return users\n",
    "\n",
    "print len(process_users('sample_sf.osm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problemchars': 10, 'lower': 63014, 'other': 1305, 'lower_colon': 34265}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Check the \"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "Expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with problematic characters.\n",
    "\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "\"\"\"\n",
    "\n",
    "# Three regular expressions to check for certain patterns in the tags\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    element_tag = element.tag\n",
    "    if element_tag == \"tag\":\n",
    "        k_attribute_value = element.get(\"k\")\n",
    "        if lower.match(k_attribute_value):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.match(k_attribute_value):\n",
    "            keys[\"lower_colon\"] +=1\n",
    "        elif problemchars.search(k_attribute_value):\n",
    "            keys[\"problemchars\"] +=1\n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "print (process_map('sample_sf.osm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': set(['W Of Us 101 @ Jct Sr 1']),\n",
      " '105': set(['Grand Avenue #105', 'N California Blvd #105']),\n",
      " '3': set(['University Dr. Suite 3']),\n",
      " '39': set(['Pier 39']),\n",
      " '4.5': set(['SF 80 PM 4.5']),\n",
      " 'A': set(['Pier 50 A']),\n",
      " 'Alameda': set(['Alameda', 'The Alameda']),\n",
      " 'Alley': set(['August Alley',\n",
      "               'Hodges Alley',\n",
      "               'Redfield Alley',\n",
      "               'Ross Alley']),\n",
      " 'Ave': set(['Esplanade Ave',\n",
      "             'Lorton Ave',\n",
      "             'Magnolia Ave',\n",
      "             'Pennsylvania Ave',\n",
      "             'Tehama Ave',\n",
      "             'Telegraph Ave']),\n",
      " 'Ave.': set(['Edes Ave.', 'Willie Stargell Ave.']),\n",
      " 'Bay': set(['Bay']),\n",
      " 'Blvd': set(['Newark Blvd',\n",
      "              'Under Ramp Sw Quad Of Us 101 / Sr 92 Ic Off 19th Ave & Fashion Island Blvd']),\n",
      " 'Bridgeway': set(['Bridgeway']),\n",
      " 'Broadway': set(['Broadway']),\n",
      " 'Center': set(['Bon Air Center', 'South Shore Center', 'Westlake Center']),\n",
      " 'Circle': set(['Blossom Circle',\n",
      "                'Citrus Circle',\n",
      "                'Columbia Circle',\n",
      "                'Gloria Circle',\n",
      "                'Inner Circle',\n",
      "                'Murray Circle']),\n",
      " 'Cres': set(['Wellesley Cres']),\n",
      " 'Ctr': set(['Linda Mar Shppng Ctr']),\n",
      " 'Cut': set(['Short Cut']),\n",
      " 'Dr': set(['Chateau Dr']),\n",
      " 'East': set(['Francisco Boulevard East']),\n",
      " 'Gardens': set(['Wildwood Gardens']),\n",
      " 'H': set(['Avenue H']),\n",
      " 'Highway': set(['Bayshore Highway',\n",
      "                 'Great Highway',\n",
      "                 'North Cabrillo Highway',\n",
      "                 'Redwood Highway']),\n",
      " 'Hwy': set(['40 Shoreline Hwy']),\n",
      " 'Las': set(['Alameda De Las']),\n",
      " 'Loop': set(['Hamlin Loop']),\n",
      " 'Mall': set(['Palos Verdes Mall']),\n",
      " 'Mason': set(['Fort Mason', 'Mason']),\n",
      " 'North': set(['Mission Bay Boulevard North']),\n",
      " 'Ora': set(['Avenue Del Ora']),\n",
      " 'Path': set(['Indian Rock Path']),\n",
      " 'Pl': set(['San Francisco/Oakland Bridge Toll Pl']),\n",
      " 'Plaza': set(['Alamo Plaza', 'El Cerrito Plaza', 'Manor Plaza']),\n",
      " 'Plz': set(['Woodside Plz']),\n",
      " 'Rd': set(['Ascot Rd']),\n",
      " 'Rd.': set(['Alpine Rd.']),\n",
      " 'Real': set(['El Camino Real', 'West El Camino Real']),\n",
      " 'St': set(['Laurel St', 'Leavenworth St', 'Park St']),\n",
      " 'St.': set(['Halleck St.']),\n",
      " 'Yolanda': set(['Corte Yolanda']),\n",
      " 'avenue': set(['Santa Cruz avenue']),\n",
      " 'st': set(['2640 mason st'])}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "- audit the OSMFILE and change the variable 'mapping' to reflect the changes needed to fix \n",
    "    the unexpected street types to the appropriate ones in the expected list.   \n",
    "- write the update_name function, to actually fix the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"sample_sf.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Circle\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Terrace\", \"Walk\", \"Way\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Dr\": \"Drive\"\n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    words = name.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in mapping:\n",
    "            words[i] = mapping[words[i]]  \n",
    "    return ' '.join(words)\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database. To do so I will parse the elements in the OSM XML file, transforming them from document format to tabular format, thus making it possible to write to .csv files. These csv files can then easily be imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "In the next piece of code I am loading the data, performing iterative parsing and writing the\n",
    "output to csv files. The shape_element function will transform each element into the correct format. To make this process easier \n",
    "__Shape Element Function__\n",
    "The function will take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "__ If the element top level tag is \"node\":__\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "__ If the element top level tag is \"way\":__\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\n",
    "To summarize, the columns in the nodes table will be:\n",
    "\n",
    "id    user    uid    version    lat    lon    timestamp    changeset\n",
    "\n",
    "Columns in the nodes_tags table will be: \n",
    "\n",
    "id    key    value    type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')  \n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema   \n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def is_street_name(full_key_name):\n",
    "    return (full_key_name == \"addr:street\")\n",
    "\n",
    "def update_street_name(name):\n",
    "    mapping = { \"St\": \"Street\", \"St.\": \"Street\", \"Rd.\": \"Road\", \"Rd\": \"Road\", \"Ave\": \"Avenue\", \"Dr\": \"Drive\"}\n",
    "    words = name.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in mapping:\n",
    "            words[i] = mapping[words[i]]  \n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_value(value, key_name, full_key_name):\n",
    "    if is_street_name(full_key_name):\n",
    "        return update_street_name(value)\n",
    "    elif 'cuisine' in key_name.lower():\n",
    "        return value.replace('_', ' ').lower()\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# ================================================== #\n",
    "#       Application Specific Helper Functions        #\n",
    "# ================================================== #\n",
    "def parse_list(tag, parent_id, default_tag_type):\n",
    "    # Parse and clean tags. \n",
    "    all_rows = []\n",
    "    if PROBLEMCHARS.search(tag.get('k')):\n",
    "        return all_rows\n",
    "    else:\n",
    "        values = tag.get('v').split(';') # example values = \"brunch;breakfast;coffee\"\n",
    "        for value in values:\n",
    "            parsed_tag = {}\n",
    "            parsed_tag['id'] = parent_id\n",
    "            k_value = tag.get('k')  # example \"addr:street\"  \"diet:vegan\"\n",
    "            idx_colon = k_value.find(':') \n",
    "            if idx_colon == -1: # example = \"cuisine\"\n",
    "                # Parse for k without ':'\n",
    "                parsed_tag['type'] = default_tag_type\n",
    "                parsed_tag['key'] = k_value\n",
    "            else:\n",
    "                parsed_tag['type'] = k_value[0:idx_colon]  # example \"addr:street\"\n",
    "                parsed_tag['key'] = k_value[idx_colon+1:]\n",
    "            parsed_tag['value'] = clean_value(value, parsed_tag['key'], k_value)\n",
    "            all_rows.append(parsed_tag)\n",
    "        return all_rows\n",
    "\n",
    "def parse_node(element, default_tag_type):\n",
    "    # Node parsing logic here. Returns node_attributes and node_tags.\n",
    "    node_attributes = { key:element.get(key) for key in NODE_FIELDS }\n",
    "    tags = []\n",
    "    for child in element.findall('tag'):\n",
    "        tag = parse_list(child, element.get('id'), default_tag_type)\n",
    "        if tag: \n",
    "            tags.extend(tag)\n",
    "    return node_attributes, tags\n",
    "    \n",
    "def parse_way_node(element, parent_id, idx):\n",
    "    return {'id':parent_id, 'node_id': element.get('ref'), 'position':idx}\n",
    "    \n",
    "def parse_way(element, default_tag_type):\n",
    "    # Way parsing logic here. Returns way_attributes, way_nodes, and way_tags.\n",
    "    way_attributes = { key:element.get(key) for key in WAY_FIELDS }\n",
    "    way_nodes = []\n",
    "    for idx, child in enumerate(element.findall('nd')):\n",
    "        way_nodes.append(parse_way_node(child, element.get('id'), idx))\n",
    "    tags = []\n",
    "    for child in element.findall('tag'):\n",
    "        tag = parse_list(child, element.get('id'), default_tag_type)\n",
    "        if tag:\n",
    "            tags.extend(tag)\n",
    "    return way_attributes, way_nodes, tags\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    # Clean and shape node or way XML element to Python dict\n",
    "    if element.tag == 'node':\n",
    "        node_attribs, tags = parse_node(element, default_tag_type)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        way_attribs, way_nodes, tags = parse_way(element, default_tag_type)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    NODES_PATH = \"nodes.csv\"\n",
    "    NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "    WAYS_PATH = \"ways.csv\"\n",
    "    WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "    WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'wb') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "process_map(\"san-francisco.osm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While uploading to database, I still found 5 rows with empty uid and username cells in the nodes csv file. So I went back and removed the rows which had empty uid cells. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53800 53801 53804 53835 53846]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import unicodecsv\n",
    "import pandas as pd\n",
    "\n",
    "all_data = pd.read_csv('nodes.csv')\n",
    "print(pd.isnull(all_data).any(1).nonzero()[0])\n",
    "\n",
    "def read_csv(filename, outputfile):\n",
    "    rows_to_remove = [53800, 53801, 53804, 53835, 53846]\n",
    "    with open(filename, 'rb') as f_read:\n",
    "        reader = unicodecsv.DictReader(f_read)\n",
    "        with open(outputfile, 'wb') as f_write:\n",
    "            writer = unicodecsv.DictWriter(f_write, reader.fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row_idx, row in enumerate(reader):\n",
    "                if row_idx not in rows_to_remove:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "read_csv('nodes.csv', 'nodes-cleaned.csv')\n",
    "\n",
    "all_data = pd.read_csv('nodes-cleaned.csv')\n",
    "print(pd.isnull(all_data).any(1).nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
